{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Fixed train.csv')\n",
    "words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 34583),\n",
       " (',', 31356),\n",
       " ('a', 25066),\n",
       " ('of', 24153),\n",
       " ('and', 23762),\n",
       " ('to', 16686),\n",
       " ('.', 13131),\n",
       " (\"'s\", 12630),\n",
       " ('in', 10128),\n",
       " ('is', 9934),\n",
       " ('that', 9071),\n",
       " ('it', 7719),\n",
       " ('as', 6340),\n",
       " ('with', 5711),\n",
       " ('for', 5445),\n",
       " ('its', 5243),\n",
       " ('film', 4953),\n",
       " ('an', 4457),\n",
       " ('movie', 4353),\n",
       " ('this', 3805),\n",
       " ('be', 3803),\n",
       " ('but', 3724),\n",
       " ('on', 3517),\n",
       " ('The', 3481),\n",
       " ('you', 3440),\n",
       " (\"n't\", 2935),\n",
       " ('by', 2874),\n",
       " ('more', 2865),\n",
       " ('his', 2783),\n",
       " ('about', 2692),\n",
       " ('from', 2576),\n",
       " ('than', 2568),\n",
       " ('at', 2566),\n",
       " ('--', 2550),\n",
       " ('or', 2547),\n",
       " ('not', 2533),\n",
       " ('one', 2526),\n",
       " ('all', 2263),\n",
       " ('have', 2260),\n",
       " ('are', 2231),\n",
       " ('like', 2137),\n",
       " (\"'\", 2124),\n",
       " ('has', 2061),\n",
       " ('A', 2024),\n",
       " ('so', 1898),\n",
       " ('story', 1824),\n",
       " ('-RRB-', 1820),\n",
       " ('out', 1758),\n",
       " ('who', 1717),\n",
       " ('most', 1638),\n",
       " ('into', 1614),\n",
       " ('too', 1575),\n",
       " ('-LRB-', 1556),\n",
       " ('up', 1539),\n",
       " ('good', 1488),\n",
       " ('their', 1438),\n",
       " ('characters', 1411),\n",
       " ('...', 1379),\n",
       " ('`', 1370),\n",
       " (\"''\", 1360),\n",
       " ('``', 1357),\n",
       " ('can', 1347),\n",
       " ('much', 1340),\n",
       " ('I', 1314),\n",
       " ('comedy', 1292),\n",
       " ('no', 1266),\n",
       " ('your', 1263),\n",
       " ('if', 1228),\n",
       " ('some', 1209),\n",
       " ('time', 1199),\n",
       " ('just', 1189),\n",
       " ('what', 1180),\n",
       " ('does', 1179),\n",
       " ('will', 1160),\n",
       " ('way', 1146),\n",
       " ('funny', 1145),\n",
       " ('little', 1123),\n",
       " ('even', 1092),\n",
       " ('any', 1073),\n",
       " ('very', 1070),\n",
       " ('life', 1056),\n",
       " ('make', 1050),\n",
       " ('only', 1045),\n",
       " ('which', 1034),\n",
       " ('been', 1027),\n",
       " ('movies', 1018),\n",
       " ('It', 994),\n",
       " ('he', 930),\n",
       " ('own', 903),\n",
       " ('was', 902),\n",
       " ('us', 897),\n",
       " ('enough', 896),\n",
       " ('work', 896),\n",
       " ('her', 893),\n",
       " ('do', 890),\n",
       " ('other', 856),\n",
       " ('they', 851),\n",
       " ('something', 851),\n",
       " ('would', 840),\n",
       " ('bad', 833)]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "for sentence in df.Phrase:\n",
    "    for words in sentence.split(' '):\n",
    "        vocab[words] += 1\n",
    "\n",
    "vocab.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing text\n",
    "def preprocessor(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentences = text.split(' ')\n",
    "    data = []\n",
    "    for word in sentences:\n",
    "        word = re.sub('<[^>]*>', '', word)\n",
    "        word = re.sub('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)','', word)\n",
    "        word = re.sub(\"n't\", 'not', word)\n",
    "        word = re.sub('\\d+','',word) #Removing digit\n",
    "        word = re.sub(r'[^\\w\\s]','',word) #Removing non-alphanumerical words \n",
    "        word = word.lower()\n",
    "        word = re.sub('(rrb)|(lrb)','', word)\n",
    "        word = lemmatizer.lemmatize(word, pos = 'v')\n",
    "        word = lemmatizer.lemmatize(word, pos = 'n')\n",
    "        data.append(word)\n",
    "    return ' '.join(data)\n",
    "\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('better', pos = 'a') #Example of lemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['Phrase']\n",
    "y = df['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le', 'make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(ngram_range=(1, 2),\n",
       "                                 preprocessor=<function preprocessor at 0x000001BF24152798>,\n",
       "                                 stop_words='english')),\n",
       "                ('clf', LogisticRegression(max_iter=700, random_state=0))])"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(stop_words = 'english', preprocessor = preprocessor,\n",
    "                       ngram_range = (1,2))\n",
    "\n",
    "clf = Pipeline([('vect', count),\n",
    "               ('clf', LogisticRegression(solver = 'lbfgs', random_state = 0, max_iter = 700))])\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6473151352044085\n"
     ]
    }
   ],
   "source": [
    "y_hat = clf.predict(X_test)\n",
    "print(accuracy_score(y_hat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('test.csv')\n",
    "prediction = clf.predict(test_set['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'Phrase': test_set['Phrase'], 'Sentiment': prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('Submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Phrase']\n",
    "y = df['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le', 'make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 TfidfVectorizer(ngram_range=(1, 2),\n",
       "                                 preprocessor=<function preprocessor at 0x000001BF24152798>,\n",
       "                                 stop_words='english', sublinear_tf=True)),\n",
       "                ('clf', LogisticRegression(max_iter=500, random_state=1))])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words = 'english', preprocessor = preprocessor,\n",
    "                       ngram_range = (1,2))\n",
    "\n",
    "clf_2 = Pipeline([('vect', tfidf),\n",
    "               ('clf', LogisticRegression(solver = 'lbfgs', random_state = 1, max_iter = 500))])\n",
    "\n",
    "clf_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6249733008671878\n"
     ]
    }
   ],
   "source": [
    "y_hat = clf_2.predict(X_test)\n",
    "print(accuracy_score(y_hat, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('cs_ftmle': conda)",
   "language": "python",
   "name": "python37864bitcsftmleconda8f70978e03294adb93c22517d5f60ab6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
